{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaf1reU6AQOf"
      },
      "source": [
        "Hemos optado por sólo codificar en inglés, pero las explicaciones y comentarios del código estarán en español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6b0gs4K-iKZ"
      },
      "outputs": [],
      "source": [
        "# Importamos todas las librerías a utilizar:\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm # Para barras de progreso, ayuda mucho ya que la API tiene delay de 1s por cada request!!\n",
        "from shapely.geometry import Point\n",
        "from shapely import wkt # Lo usaremos para parsear algunas columnas de algunos datasets a geometry, en un GDF.\n",
        "import geemap,ee\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYl6tJA4PEbB"
      },
      "outputs": [],
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize(project='ee-aesmatias')\n",
        "#Aqui, por google colab no tuve que usar un token ni nada, pero se requiere autenticacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wqT6Q5pQGd2"
      },
      "source": [
        "Descargamos el Shapefile de USA desde https://gadm.org/download_country.html, eligiendo United States y descomprimiendo el .zip, eso nos dará los archivos necesarios, luego cargamos el Shapefile de USA y filtramos el AOI en Manhattan, para finalmente de transformarlo a GEOJSON y poder utilizarlo en GEE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "664WxEU8QKRE"
      },
      "outputs": [],
      "source": [
        "# Nivel 2 para elegir los condados, luego lo pasamos a EPSG:4326, compatible con GEE\n",
        "gdf_USA = gpd.read_file(\"gadm41_USA_2.shp\").to_crs(epsg=4326)\n",
        "\n",
        "ny_TO_GDF = gdf_USA[gdf_USA['NAME_2'] == 'New York'] # Seleccionamos New York\n",
        "ny_TO_GDF.to_file(\"manhattan.geojson\", driver=\"GeoJSON\") # Hacemos un .geojson y lo guardamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dF4zmYQSSLx"
      },
      "outputs": [],
      "source": [
        "# Cargamos el geojson creado, para que GEE lo pueda utilizar\n",
        "gdf = gpd.read_file(\"manhattan.geojson\")\n",
        "manhattan_geojson = json.loads(gdf.to_json())\n",
        "manhattan_ee = ee.FeatureCollection(manhattan_geojson)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvfjXaAHUpXI"
      },
      "source": [
        "Visualizamos manhattan con GEE, podemos ajustar los parámetros como la opacidad del AOI en el mapa interactivo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpxhsKuLRQYq"
      },
      "outputs": [],
      "source": [
        "manhattanCollection = (ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "    .filterBounds(manhattan_ee)\n",
        "    .filterDate(\"2024-05-01\", \"2025-04-01\") # Mayo 2024 - Abril 2025\n",
        "    .median() # Usamos la mediana de las imágenes de momento, sólo queremos apreciar el mapa\n",
        "    .clip(manhattan_ee)) # Recortamos en Manhattan, la AOI\n",
        "\n",
        "Map = geemap.Map(center=[40.783, -73.971], zoom=12)\n",
        "\n",
        "Map.addLayer(manhattanCollection, {\n",
        "    'bands': ['B4', 'B3', 'B2'],\n",
        "    'min': 0,\n",
        "    'max': 3000\n",
        "}, 'RGB')\n",
        "\n",
        "Map.addLayer(manhattan_ee.style(**{\n",
        "    'width': 1,\n",
        "    'color': 'red', # El borde será CYAN\n",
        "    #'fillColor': '00000000',  # Color transparente de relleno\n",
        "}), {}, 'AOI Manhattan')\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6oi1iEb_D6S"
      },
      "source": [
        "Se ha utilizado el dataset de Manhattan, obtenido en https://www.nyc.gov/site/finance/property/property-rolling-sales-data.page, la fecha de los registros del dataset está entre Mayo 2024 - Abril 2025."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM8Dd5KBLAhL"
      },
      "source": [
        "Aquí, en las celdas iniciales que siguen, se muestra como se han geocodificado algunos datos con ayuda de una API, pero no hace falta usarlo, porque los datos procesados ya han sido guardados en un .csv (rollingsales_manhattan_geocoded.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYgztuOHI0EU"
      },
      "outputs": [],
      "source": [
        "'''La API que usamos para geocodificar en su capa gratuita es para testing, este proyecto\n",
        "Universitario no califica como un proyecto de producción, no hay usuario final que lo utilizará.\n",
        "Además, sólo hicimos uso de la API en pocas ocasiones'''\n",
        "\n",
        "OPENCAGE_API_KEY = 'KEY'\n",
        "OPENCAGE_BASE_URL = 'https://api.opencagedata.com/geocode/v1/json'\n",
        "\n",
        "input_excel_file = './rollingsales_manhattan.xlsx' # XLSX (excel)\n",
        "\n",
        "# Output y failed logs, para poder tener una reanudación cada 2500 request en la geocodificación\n",
        "output_csv_file = './rollingsales_manhattan_geocoded.csv'\n",
        "failed_addresses_file = './rollingsales_manhattan_geocoding_failures.csv'\n",
        "\n",
        "MAX_DAILY_REQUESTS = 2500\n",
        "REQUEST_DELAY = 1.0 # seconds\n",
        "\n",
        "REQUIRED_COLUMNS = ['ADDRESS', 'BOROUGH', 'ZIP CODE', 'SALE PRICE']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHgsut25JEsu"
      },
      "source": [
        "Creamos la función que procesará cada solicitud a la API para poder geocodificar las direcciones y los ZIP codes en coordenadas geográficas que usaremos en los GDF posteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuDChyt3I0bJ"
      },
      "outputs": [],
      "source": [
        "# Esta función hace un llamado a la API, y retorna un array con la latitud, longitud y estado\n",
        "def geocode_address(address_str, api_key, borough=None, zip_code=None):\n",
        "    query = f\"{address_str}, {borough}, New York, NY {zip_code}\" if borough and zip_code else address_str\n",
        "\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'key': api_key,\n",
        "        'language': 'en',\n",
        "        'no_annotations': 1,\n",
        "        'limit': 1\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(OPENCAGE_BASE_URL, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if data and data['results']:\n",
        "            lat = data['results'][0]['geometry']['lat']\n",
        "            lng = data['results'][0]['geometry']['lng']\n",
        "\n",
        "            components = data['results'][0].get('components', {})\n",
        "            is_nyc = False # Empieza como false, y si lo encontramos, lo cambiamos a True:\n",
        "            if 'state_code' in components and components['state_code'] == 'NY':\n",
        "                if 'city' in components and components['city'] in ['New York', 'Brooklyn', 'Queens', 'Bronx', 'Staten Island', 'Manhattan']:\n",
        "                    is_nyc = True\n",
        "                elif 'county' in components and ('New York County' in components['county'] or \\\n",
        "                                                 'Kings County' in components['county'] or \\\n",
        "                                                 'Queens County' in components['county'] or \\\n",
        "                                                 'Bronx County' in components['county'] or \\\n",
        "                                                 'Richmond County' in components['county']):\n",
        "                    is_nyc = True\n",
        "\n",
        "            if is_nyc:\n",
        "                return lat, lng, \"Success\"\n",
        "            else:\n",
        "                return None, None, \"Not_NYC_Result\"\n",
        "        else:\n",
        "            return None, None, \"No_Results\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        if response.status_code == 429:\n",
        "            return None, None, \"Rate_Limit_Exceeded\"\n",
        "        elif response.status_code == 401:\n",
        "            print(f\"API Key Error!!\")\n",
        "            return None, None, \"API_Key_Error\"\n",
        "        elif response.status_code == 402: # Entonces, llegamos al límite de la quota diaria gratis\n",
        "            print(f\"ERROR, Quota exceded!\")\n",
        "            return None, None, \"Payment_Required_Error\"\n",
        "        else:\n",
        "            print(f\"Request error: {e}\")\n",
        "            return None, None, f\"Request_Error: {e}\"\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        return None, None, f\"Error: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yyyEAuxJYMY"
      },
      "source": [
        "Comenzamos a procesar las direcciones y ZIP codes a coordenadas geográficas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGnrrVuQ34uk"
      },
      "outputs": [],
      "source": [
        "print(f\"*** Procesando {input_excel_file} ***\")\n",
        "\n",
        "df = None\n",
        "# Agregamos las columnas requeridas que fallaron a entradas del df fallido, para re intentar si se quisiera:\n",
        "failed_df = pd.DataFrame(columns=REQUIRED_COLUMNS + ['Reason'])\n",
        "\n",
        "try:\n",
        "    if pd.io.common.file_exists(output_csv_file):\n",
        "        print(f\"Fichero '{output_csv_file}' con progreso encontrado - Resumiendo...\")\n",
        "        df = pd.read_csv(output_csv_file)\n",
        "        # Si no hay latitud, longitud o estado de la geocodificación, sabemos que la entrada no ha sido procesada:\n",
        "        for col in ['LATITUDE', 'LONGITUDE', 'GEOCODING_STATUS']:\n",
        "            if col not in df.columns:\n",
        "                df[col] = None\n",
        "            df[col] = df[col].astype(object)\n",
        "\n",
        "    else:\n",
        "        print(f\"No se encontró el fichero con progreso, cargando el fichero inicial XLSX: '{input_excel_file}'.\")\n",
        "        found_header = False\n",
        "        # Probamos con headers desde el 0 al 10, porque algunos datasets XLSX tienen las primeras entradas con información,\n",
        "        # hay que evitar las primeras entradas que no son los headers, para no obtener errores:\n",
        "        for header_row_index in range(10):\n",
        "            try:\n",
        "                print(f\"Intentando cargar con el header {header_row_index}\")\n",
        "                df_temp = pd.read_excel(input_excel_file, header=header_row_index)\n",
        "\n",
        "                # Check if all required columns are present in the loaded DataFrame\n",
        "                if all(col in df_temp.columns for col in REQUIRED_COLUMNS):\n",
        "                    df = df_temp\n",
        "                    found_header = True\n",
        "                    print(f\"Header encontrado en el índice {header_row_index}!\")\n",
        "                    print(\"Columnass encontradas en el DF:\", df.columns.tolist())\n",
        "                    break # Si el header es encontrado, dejamos de loopear\n",
        "                else:\n",
        "                    print(f\"No hay header en el índice {header_row_index}, probando el siguiente...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "\n",
        "        if not found_header: # Si no hay header, hay un error en el fichero\n",
        "            print(f\"No se ha encontrado un header, error en el fichero XLSX\")\n",
        "            exit()\n",
        "\n",
        "        # Limpiamos las columnas del DF y las parseamos\n",
        "        df['ADDRESS'] = df['ADDRESS'].fillna('').astype(str)\n",
        "        df['NEIGHBORHOOD'] = df['NEIGHBORHOOD'].fillna('').astype(str)\n",
        "        df['BOROUGH'] = df['BOROUGH'].fillna('').astype(str)\n",
        "        df['ZIP CODE'] = df['ZIP CODE'].fillna(0).astype(int).astype(str).replace('0', '')\n",
        "\n",
        "        # Agregamos la latitud, longitud, y estado de la geocodificación, como nuevas columnas en el DF:\n",
        "        df['LATITUDE'] = None\n",
        "        df['LONGITUDE'] = None\n",
        "        df['GEOCODING_STATUS'] = None\n",
        "\n",
        "    already_geocoded_count = df['LATITUDE'].notna().sum()\n",
        "    requests_made_now = 0\n",
        "    print(f\"Los registros geocodificados hasta el momento son: {already_geocoded_count}\")\n",
        "\n",
        "    # La siguiente línea veririfica si la latitud está vacía y el \"status\" es diferente de \"Not_NYC_Result\"\n",
        "    # Si el registro tiene GEOCODIG_STATUS = 'Not_NYC_Result', entonces ha fallado la geocodificación.\n",
        "    rows_to_geocode = df[(df['LATITUDE'].isna()) & (df['GEOCODING_STATUS'] != 'Not_NYC_Result')]\n",
        "\n",
        "    print(f\"Han fallado: {len(rows_to_geocode)} registros\")\n",
        "\n",
        "    for index, row in tqdm(rows_to_geocode.iterrows(), total=len(rows_to_geocode), desc=\"Geocoding\"): #tqdm para barra de progrso\n",
        "        if requests_made_now >= MAX_DAILY_REQUESTS:\n",
        "            print(f\"Se ha llegado al límite de {MAX_DAILY_REQUESTS} requests diarias!\")\n",
        "            break\n",
        "\n",
        "        address = row['ADDRESS']\n",
        "        borough = row['BOROUGH']\n",
        "        zip_code = row['ZIP CODE'] if row['ZIP CODE'] != '0' else ''\n",
        "\n",
        "        if not address:\n",
        "            df.loc[index, 'GEOCODING_STATUS'] = \"Empty_Address\"\n",
        "            continue\n",
        "\n",
        "        lat, lon, status = geocode_address(address, OPENCAGE_API_KEY, borough, zip_code)\n",
        "        requests_made_now += 1\n",
        "\n",
        "        df.loc[index, 'LATITUDE'] = lat\n",
        "        df.loc[index, 'LONGITUDE'] = lon\n",
        "        df.loc[index, 'GEOCODING_STATUS'] = status\n",
        "\n",
        "        if status in [\"Rate_Limit_Exceeded\", \"API_Key_Error\", \"Payment_Required_Error\"]:\n",
        "            break\n",
        "\n",
        "        time.sleep(REQUEST_DELAY) # La documentación de la API indica un delay de 1 segundo entre cada request\n",
        "\n",
        "        # Guardamos el progreso en chunks de cada 100 solicitudes a la API:\n",
        "        if requests_made_now % 100 == 0:\n",
        "            print(f\"Guardando progreso en la request número {requests_made_now}\")\n",
        "            df.to_csv(output_csv_file, index=False)\n",
        "\n",
        "    df.to_csv(output_csv_file, index=False)\n",
        "    print(f\"Geocodificación finalizada!!\")\n",
        "\n",
        "    failed_rows = df[df['LATITUDE'].isna()] # Si no tiene latitud, es una row fallida\n",
        "    if not failed_rows.empty:\n",
        "        failed_df_to_save = failed_rows[['BOROUGH', 'NEIGHBORHOOD', 'ADDRESS', 'ZIP CODE', 'GEOCODING_STATUS']].copy()\n",
        "        # Guardamos en failed_addresses_file sólo las rows de la variable de arriba, para poder procesarlas luego.\n",
        "        failed_df_to_save.to_csv(failed_addresses_file, index=False)\n",
        "        print(f\"Los registros fallidos se guardaron en: {failed_addresses_file}\")\n",
        "    else:\n",
        "        print(\"Atención! Finalización inesperada, posiblemente ha ocurrido un error.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Archivo '{input_excel_file}' no hallado.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHyojXTMXc6L"
      },
      "source": [
        "Cargamos el fichero con la latitud y longitud agregadas en la geocodificación, para luego, filtrar los registros que tengan NaN en latitud y longitud, ya que eso es producto de errores en la geocodificación de dichos valores. También cribamos y sólo tomamos como válidos valores con precio de venta mayor a 0, ya que hay varios valores en el dataset con propiedades que se han vendido a costo 0, lo que no tiene representación estadística en nuestro contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxJdB0e94hCo"
      },
      "outputs": [],
      "source": [
        "properties_geocoded_file = './rollingsales_manhattan_geocoded.csv'\n",
        "\n",
        "try:\n",
        "    df_properties = pd.read_csv(properties_geocoded_file)\n",
        "    print(f\"***Cargando fichero: {properties_geocoded_file}*** \\n\")\n",
        "    print(f\"Cantidad de registros encontrados en {str(properties_geocoded_file)}: {len(df_properties)} \\n\")\n",
        "    print(\"df_properties.info(): \\n\")\n",
        "    print(df_properties.info())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    exit()\n",
        "\n",
        "# Eliminamos los registros que sean NaN en latitud y longitud.\n",
        "df_properties.dropna(subset=['LATITUDE', 'LONGITUDE'], inplace=True)\n",
        "print(f\"Registros después de eliminar NaN en lat/lon: {len(df_properties)}\")\n",
        "\n",
        "#Eliminamos los registros que tengan precio de venta menor o igual a 0, sin representación estadística.\n",
        "df_properties = df_properties[df_properties['SALE PRICE'] > 0]\n",
        "print(f\"Registros después de filtrar por precio de venta > 0: {len(df_properties)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1kLfLf6cMl5"
      },
      "source": [
        "Luego, para una posterior manipulación y tener una mayor compatibilidad con GEE y librerías para graficar, \"transformamos\" el dataframe a un geodataframe, y le agregamos una nueva columna de tipo geometry, que contendrá puntos generados a través de las propiedades de latitud y longitud, los cuales están definidos en la variable geometry, haciendo uso del métetodo zip y list comprehension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSAhuFKN4lFW"
      },
      "outputs": [],
      "source": [
        "# A partir de las coordenadas, creamos objetos de tipo Point, de lalibrería shapely.geometry, para luego manipular mejor:\n",
        "geometry = [Point(xy) for xy in zip(df_properties['LONGITUDE'], df_properties['LATITUDE'])]\n",
        "\n",
        "# Creamos el nuevo geodataframe para, a partir del dataframe anterior, llenarlo con los datos:\n",
        "gdf_properties = gpd.GeoDataFrame(df_properties, geometry=geometry, crs=\"EPSG:4326\") #CRS es EPSG:4326 para latitud y longitud.\n",
        "\n",
        "print(\"Nuevo GeoDataFrame:\")\n",
        "print(gdf_properties.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgWEfpVligZX"
      },
      "source": [
        "Cargamos el dataset https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic que contiene el histórico de tiroteos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT1--5td5E3i"
      },
      "outputs": [],
      "source": [
        "shooting_incident_historic = 'NYPD_Shooting_Incident_Data__Historic_.csv'\n",
        "df_shooting_incident_historic = None\n",
        "\n",
        "try:\n",
        "    df_shooting_incident_historic = pd.read_csv(shooting_incident_historic)\n",
        "    print(f\"El archivo {shooting_incident_historic} ha sido cargado\\n Longitud: {len(df_shooting_incident_historic)} \\n\")\n",
        "    print(f'Información del fichero: \\n')\n",
        "    df_shooting_incident_historic.info()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjdic-wQqglJ"
      },
      "source": [
        " Al igual que antes, este dataset también debe ser filtrado por fecha de interés y ser convertido a GDF, la columna con el Point, que contiene la geometría de la latitud y longitud, está en una columna llamada \"Lon_Lat\", y contiene valores de tipo Point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RFXeYKI5Spj"
      },
      "outputs": [],
      "source": [
        "LATITUDE_COL = 'Latitude'\n",
        "LONGITUDE_COL = 'Longitude'\n",
        "INCIDENT_DATE_COL = 'OCCUR_DATE'\n",
        "BORO_COL = 'BORO'\n",
        "year_of_preference = 2024 # Nos interesan datos de tiroteos en los últimos 2 años\n",
        "\n",
        "# Cargamos el archivo y definimos una variable para su dataframe\n",
        "shooting_incident_historic = 'NYPD_Shooting_Incident_Data__Historic_.csv'\n",
        "\n",
        "try:\n",
        "    df_shooting_incident_historic = pd.read_csv(shooting_incident_historic)\n",
        "    print(f\"El archivo {shooting_incident_historic} ha sido cargado. Su longitud es: {len(df_shooting_incident_historic)} \\n\")\n",
        "    df_shooting_incident_historic.info()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Eliminamos las filas con NaN en las coordenadas\n",
        "df_shooting_incident_historic.dropna(subset=[LATITUDE_COL, LONGITUDE_COL], inplace=True)\n",
        "\n",
        "# Si la columna con fecha del incidente existe, filtramos por BORO y AÑO:\n",
        "if INCIDENT_DATE_COL in df_shooting_incident_historic.columns:\n",
        "    # Convertimos la columna de OCCUR_DATE a datatime de pandas, para trabajarla con el formato de USA:\n",
        "    df_shooting_incident_historic[INCIDENT_DATE_COL] = pd.to_datetime(\n",
        "        df_shooting_incident_historic[INCIDENT_DATE_COL],\n",
        "        format='%m/%d/%Y',\n",
        "        errors='raise' # Podemos poner coerce\n",
        "    )\n",
        "    df_shooting_incident_historic.dropna(subset=[INCIDENT_DATE_COL], inplace=True)   # Dropeamos valores sin fecha válida.\n",
        "    df_shooting_incident_historic['INCIDENT_YEAR'] = df_shooting_incident_historic[INCIDENT_DATE_COL].dt.year\n",
        "\n",
        "    # Filtramos para sólo obtener datos de Manhattan (en la col BORO)\n",
        "    if BORO_COL in df_shooting_incident_historic.columns:\n",
        "        # Usamos .copy() para definir la variable por valor, y no por referencia en memoria:\n",
        "        df_shooting_incident_manhattan = df_shooting_incident_historic[df_shooting_incident_historic[BORO_COL] == 'MANHATTAN'].copy()\n",
        "        print(f\"Encontramos: {len(df_shooting_incident_manhattan)} incidentes en Manhattan\")\n",
        "\n",
        "        # Filtramos según el año deseado:\n",
        "        df_shooting_incident_manhattan = df_shooting_incident_manhattan[df_shooting_incident_manhattan['INCIDENT_YEAR'] >= year_of_preference]\n",
        "        print(f\"Encontramos: {len(df_shooting_incident_manhattan)} incidentes posteriores al año {year_of_preference}. \\n\")\n",
        "\n",
        "        # Antes de convertir el DF a GDF, necesitamos col geometry que contiene puntos, los cuales están en la\n",
        "        # columna Lon_Lat, por lo que la parseamos:\n",
        "        df_shooting_incident_manhattan['Lon_Lat'] = df_shooting_incident_manhattan['Lon_Lat'].apply(wkt.loads)\n",
        "\n",
        "        # Creamos el GeoDataFrame de los incidentes y lo asignamos en una nueva variable:\n",
        "        geometry_incidents = [Point(xy) for xy in zip(df_shooting_incident_manhattan[LONGITUDE_COL], df_shooting_incident_manhattan[LATITUDE_COL])]\n",
        "\n",
        "\n",
        "        # Renombramos la columna Lon_Lat a geometry, ya que sabemos que existe Lon_Lat, que es un tipo de dato Point\n",
        "        df_shooting_incident_manhattan.rename(columns={'Lon_Lat': 'geometry'}, inplace=True)\n",
        "\n",
        "        # Creamos el GDF utilizando la col geometry:\n",
        "        gdf_incidents = gpd.GeoDataFrame(df_shooting_incident_manhattan, geometry='geometry', crs=\"EPSG:4326\")  # CRS 4326 para lat/lon\n",
        "\n",
        "        print(\"Información del GDF:\")\n",
        "        gdf_incidents.info()\n",
        "    else:\n",
        "      raise KeyError(f\"Error: La columna '{BORO_COL}' no se encontró!! \")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
